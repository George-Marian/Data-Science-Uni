{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Discovery Process Exercise\n",
    "\n",
    "<!-- \n",
    "author: \"Geczi-Kis Georgian\"\n",
    "date: \"2024-03-06\" \n",
    "-->\n",
    "\n",
    "Describe in max 1 page\n",
    "What is a Knowledge Discovery (KDD) Process? Please describe each step of the process.\n",
    "\n",
    "How does the spam filter of your email account work? Does it use machine learning? Statistics? Elaborate on this with the input from this lesson.\n",
    "\n",
    "Describe your spam filter using the KDD steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## What is KDD Process? \n",
    "\n",
    "Data mining, or Knowledge Discovery from Data, is the process where we extract interesting, useful and not-so obvious patterns or informations from large amounts of data. \n",
    "\n",
    "Besides the slides of the lecture, the notes from the lecture and my common sense, I also used this material for a better understanding: \n",
    "https://www.geeksforgeeks.org/kdd-process-in-data-mining/\n",
    "\n",
    "The steps of the KDD Process are:\n",
    "\n",
    "### 0. Having Data & A goal\n",
    "Obviously, to do data mining, we need the data itself, and also a goal, task or hypothesis, a range of oportunities, that the possible patterns and informations we will find, to fit. And besides this, we will need the technology necessary (hard and soft).\n",
    "\n",
    "### 1. Data Cleaning\n",
    "In this step, we need to take care of the data. We might find irrelevant/noisy data, missing values, we also would like to keep an order of data, in the time while we understand the data we are working on. \n",
    "\n",
    "### 2. Data Integration\n",
    "This step depends on how many sources of data we have. If we have multiple sources, we will need to merge them in a way that we keep consistency of the data. We might need to return to first step in case data is not \"cleaned\" or structurated well enough to keep consistency in the building of the data. \n",
    "\n",
    "### 3. Data Selection \n",
    "We got a big bunch of clean data. Focusing on the task, we most probably need to select only some parts of the data. The decision of which data we select depends on the task we have and on what we believe might be relevant for what we want to achieve. We could use decision trees, clustering or neural networks to understand what is relevant, basically to explore the data a bit. I am not certain if it is possible and if I could call somebody lucky if all the data they have is relevant, but selection is an important step to achieve some relevant knowledge. \n",
    "\n",
    "\n",
    "### 4. Data Mining \n",
    "The most important step, is where we apply various techniques to extract patterns, values, or any information that might be potentially useful. The possible ways to approach the data is through pattern discovery, association and correlation, classification, regression and/or clustering. Depending on the task and the data, we might do descriptive and/or predictive data mining. Another aspect of data mining is to considerate how complex an approach should be, as we would like to keep it the most simple possible for the task we have. \n",
    "\n",
    "### 5. Pattern Evaluation\n",
    "From the previous step, we get a set of values and/or characteristics that show us possible patterns identified in the data. Based on different criterias, we look of interesting data that might present something unknown to us, or sets of values that are visualised as useful correlations. This highly depends on what are we looking for. \n",
    "\n",
    "### 6. Knowledge Visualisation\n",
    "In this step we present the summarised data we found in a way that we can maximize its meaningfulness, so we can make decisions for the task we had from the first steps \n",
    "\n",
    "### Other info\n",
    "- Data might need to be refined\n",
    "- Steps can occur in loops where needed, for optimal results\n",
    "- It can be divided in pre-processing, processing (data mining) and post-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the spam filter of your email account work? Does it use machine learning? Statistics?\n",
    "\n",
    "First, I wanted to have some categories for technologies that filter spam emails. I will use the categories of spam filters form this website (https://mailtrap.io/blog/spam-filters/), to classify Gmail (the ESP (email service provider) I have for personal use, Outlook is only for Univerity related matters)\n",
    "\n",
    "## Types of spam Filters based on:\n",
    "# I. Deployment\n",
    "1. On-premise (gatewaty)\n",
    "    - They use physical devices\n",
    "    - Advantage: Confidentiality, Control for admins, easier to troubleshoot\n",
    "    - Disadvantages: Expensive, Hard to Scale, Unflexible\n",
    "2. Software-based or client-based\n",
    "    - Software that are on somebody's machine, hardware-dependent\n",
    "    - Advantage: Security, Customizable\n",
    "    - Disadvantage: Outdated, Client-side filtering\n",
    "\n",
    "# II. Factors they analyse\n",
    "1. Content (statistical)\n",
    "    - Analyse the content of the email, all parts, including vocabulary\n",
    "    - Advantage: Quick for classic spam, can be improved\n",
    "    - Disadvantage: might block important emails \n",
    "2. Header (statistical)\n",
    "    - Analyse meta data of the email, like number of recipients, legitimacy of email domain, IP address\n",
    "    - Advantage: might spot good spammers\n",
    "    - Disadvantage: can slip through if meta is correct\n",
    "3. Blacklist\n",
    "    - check the IP in DNSBL (Domain Name System blocklist)\n",
    "    - Advantage: keep away known spammers, frequently updated \n",
    "    - Disadvantage: if not in DNSBL, will slip through\n",
    "4. Machine Learning algorithms\n",
    "    - commonly used, supervised machine-learning methods\n",
    "    - are trained on datasets, making predictions\n",
    "    - Advantage: 98% accuracy, highly customizable, can be improved\n",
    "    - Disadvantage: bayesian poisoning (spammers use random legitimate words to confuse filters), can't detect text on images or letters substituted with characters, struggle with multilingual spam detection\n",
    "5. Rule Based\n",
    "    - filter messages on pre-defined rules\n",
    "    - Advantage: block emails automatically, highly customizable\n",
    "    - Disadvantage: narrow \n",
    "6. Language and country (statistical)\n",
    "    - block emails in different languages or from different countries\n",
    "    - Advantage: easy to implement, customizable\n",
    "    - Disadvantage: narrow, insufficient, might block important emails\n",
    "7. Source authentication filters\n",
    "    - check the authentication protocols of sender's domain\n",
    "    - Advantage: easy to implement, high accuracy, quickly\n",
    "    - Disadvantage: can't detect spam if domains are verified\n",
    "8. Challenge-response\n",
    "    - spammers usually use invalid return path email addresses or send emails in bulk, and this filters send a reply containing a specific challenge to the sender, like CAPTCHA, clicking a link, or unaltered reply\n",
    "    - Advantage: filter classic spam emails \n",
    "    - Disadvantage: no longer necessary after authentification protocols arrised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe your spam filter using the KDD steps\n",
    "\n",
    "We will analyse Gmail to see how it deals with filtering spam emails, assuming the ML is already trained. Gmail used machine learning powered by user feedback to catch spam and identify patterns in large data sets.\n",
    "1. Data Cleaning\n",
    "    - As Google has an imense data source, the data from users is took from a server\n",
    "    - I believe that the data is already stored in a way that the cleaning of the data is minimized \n",
    "2. Data Integration\n",
    "    - All the cleaned data from a set of servers (Data Warehouse) is merged in one big chunk of clean data\n",
    "3. Data Selection\n",
    "    - Depending on the methods the ML will use to filter an email, it might select all the data relevant (contents, metadata, IPs and Domains, Sender data, authentification protocols, etc.), so it can use all the types of filtering in the most efficient way (that is why Google as 99.9% rate of identifing spam, phishing, and malware emails, as found here: https://workspace.google.com/blog/identity-and-security/an-overview-of-gmails-spam-filters)\n",
    "4. Data Mining\n",
    "    - This is the part where ML process all the data given. ML might use Naive Bayes, Support Vector Machines, Decision Trees, Random Forests and Neutral Networks. The trained model will take the emails and check if they find something unusual, and try to classify them as spam or not spam. For high optimisation, it might run different tests on them using different ML algorithms to avoid false positives or false negatives. It might focus to minimize the false negatives, as we don't want to have marked as spam an email that is not actually spam.\n",
    "    - At the end, we should have a prediction if an email is spam or not spam.\n",
    "5. Pattern Evaluation\n",
    "    - The system will evaluate the quality of its predictions, as the system's focus is on classifying if an email is spam or not, and learn from it\n",
    "6. Knowledge Visualisation\n",
    "    - The visualisation will use those values that are describing the status, correctness and summary of the data\n",
    "    - I am not sure, but maybe the visualisation is (also) made in the inbox of the users (and the spam section). This will also mean the users can provide feedback, in the case of False Positives/Negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe a generic spam filter using KDD steps. \n",
    "\n",
    "1. Data Cleaning\n",
    "    - First, we begin by cleaning the data available: handle missing values, remove duplicates, or deal with any inconsistencies that might be found in the data\n",
    "    - We tabularize the data based on their contents: meta, header, content, sender's domain, IP (that is included in meta, but these are important to have easy access in my opinion)\n",
    "    - We might also need to format the contents of the mails, or to keep count of the formatting (coloured fonts, capitalization, bold/italics), and of the images, documents and links attached to them (as they might be signs of spams â€” we can store some data of how many links, images, capitalized letters, emojis, or formatted text are in an email)\n",
    "\n",
    "2. Data Integration\n",
    "    - If we have multiple sources, we need to combine the data in one dataset and keep consistency. \n",
    "\n",
    "3. Data Selection\n",
    "    - We prepare the data that will be analysed/ mined. At this step, we should already know what algorithm we will use or what parts of an email we will analyse. \n",
    "    - We will select from all the (probably) tabularized data, the ones that might be relevant in a spam email (and where we might be able to see a difference between spam and non spam). We might not be interested in font style, the length of the email, HTML tags (if any), sender's display name (email yes, display name not really), or the timestamp the email was sent.\n",
    "    - I am not sure how the common algorithms work, but I believe that we can analyse a dataset in parallelism: a type of algorithm will analyse the meta, while another analyses the content (for efficiency). If, in real life, a single algorithm will deal better with all the data, it is understandable, but I took this possibility in consideration! \n",
    "    - It is important to select and prepare the data properly for the algorithm(s) that you will use \n",
    "\n",
    "4. Data Mining\n",
    "    - we decide if we need validation data, because first thing is to divide the data and choose how. Depending on the data set, we will divide data into training, test, and if wanted, validation. We might use parallelism for K-cross validation if we have a small dataset, to validate the trained model and tune our hyperparameters. We don't want to overfit it, and also we want to work enough on it, so it won't get underfit. \n",
    "    - Usage of the chosen algorithm: Naives Bayes, Decision Trees, Neural Networks or Support Vector Machines, or which one is more suitable\n",
    "    - From my research and try-to-understand-not-really-yet-ing, i think Naive Bayes might be good for all kind of data, and also if we have a lot of data, we could use neural networks. Support vector machines need a careful validation/ tuning, and if that is not made properly (small dataset might be a reason), you might get into a lot of false positive and negatives when the model is tested or deployed (overfitted I think?)\n",
    "    - we train the model, validate and tune the hyperparameters, and then test it. Let's assume that it learned patterns and relationships within the data\n",
    "\n",
    "5. Pattern Evaluation\n",
    "    - we can use precision, recall or accuracy to evaluate how well our model performed. \n",
    "    - If the evaluation is not what we wanted (false results), we might fine-tune the model again. I am not sure how. \n",
    "\n",
    "6. Knowledge Visualisation\n",
    "    - We can visualise patterns, model's performance (the insights from pattern evaluation), and the rate of desirable results (how many true responses, how many false, negative, positive)\n",
    "\n",
    "7. *Deployment, Updates and Monitoring (*- new added)\n",
    "    - Let's assume that our system works as expected. Good job! We need to maintain this quality after deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
